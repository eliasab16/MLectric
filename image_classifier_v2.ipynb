{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eliasab16/MLectric/blob/main/image_classifier_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1gMmL1TVDN4",
        "outputId": "0c59d7b4-2020-4d19-8519-e9c57ca7ab93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/Othercomputers/My\\ MacBook\\ Pro/tf-od/panels/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "id": "TD1-RwOfICBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPZ5Q8hbdIJx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, BatchNormalization, Dropout\n",
        "from tensorflow.keras.regularizers import L1L2\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import json\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4xwE_pG8y7C"
      },
      "outputs": [],
      "source": [
        "CLASSIFIER_PATH = os.path.join('workspace/classifier')\n",
        "\n",
        "paths = {\n",
        "    'TRAINED_MODELS': os.path.join(CLASSIFIER_PATH, 'trained_models')\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgCUuR8Z2mZp"
      },
      "source": [
        "## Data prep and augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MCc_G2F5RIh"
      },
      "source": [
        "### Convert data into table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRMAAb0k2lri"
      },
      "outputs": [],
      "source": [
        "label_names = {\n",
        "    0: 'af', 1: 'bhaat2x40', 2: 'bhaat4x40', 3: 'breaker3x', 4: 'm1x6', 5: 'm1x10', 6: 'm1x16',\n",
        "    7: 'm1x20', 8: 'm1x25', 9: 'm1x32', 10: 'm1x40', 11: 'm3x6', 12: 'm3x10', 13: 'm3x16',\n",
        "    14: 'm3x20', 15: 'm3x25', 16: 'm3x32', 17: 'm3x40', 18: 'm3x50', 19: 'm3x63',\n",
        "    20: 'ms'\n",
        "}\n",
        "\n",
        "label_nums = {\n",
        "    'af': 0, 'bhaat2x40': 1, 'bhaat4x40': 2, 'breaker3x': 3, 'm1x6': 4, 'm1x10': 5, 'm1x16': 6,\n",
        "    'm1x20': 7, 'm1x25': 8, 'm1x32': 9, 'm1x40': 10, 'm3x6': 11, 'm3x10': 12, 'm3x16': 13,\n",
        "    'm3x20': 14, 'm3x25': 15, 'm3x32': 16, 'm3x40': 17, 'm3x50': 18, 'm3x63': 19,\n",
        "    'ms': 20\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a7sqPgI5dqV"
      },
      "outputs": [],
      "source": [
        "# Combine images, categories, and labels in a table\n",
        "\n",
        "data_table = {'image_path': [], 'label': []}\n",
        "root_folder = \"workspace/classifier/data\"\n",
        "\n",
        "for label_name in os.listdir(root_folder):\n",
        "    label = label_nums[label_name]\n",
        "    folder_path = os.path.join(root_folder, label_name)\n",
        "\n",
        "    for image_file in os.listdir(folder_path):\n",
        "        if image_file.lower().endswith('.jpg'):\n",
        "            image_path = os.path.join(folder_path, image_file)\n",
        "            data_table['image_path'].append(image_path)\n",
        "            data_table['label'].append(label)\n",
        "\n",
        "# Create a Pandas DataFrame from the data dictionary\n",
        "original_df = pd.DataFrame(data_table)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(original_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-beCGGau84e"
      },
      "source": [
        "### Agumentation #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFvX8JAXJ9PQ"
      },
      "source": [
        "#### Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8HlfHA0JzKZ"
      },
      "outputs": [],
      "source": [
        "augmentation_generator = ImageDataGenerator(\n",
        "    rotation_range=0.8,\n",
        "    height_shift_range=0.08,\n",
        "    width_shift_range=0.12,\n",
        "    rescale=1.0/255.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiBmi65XRZ_7"
      },
      "outputs": [],
      "source": [
        "augmentation map = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE58ucYAvAQo"
      },
      "outputs": [],
      "source": [
        "augmented_data = []\n",
        "root_folder = \"workspace/classifier/data\"\n",
        "\n",
        "for (target_label, iterations) in augmentation_map:\n",
        "  filtered_table = original_df[original_df['label'] == target_label]\n",
        "\n",
        "  for _ in range(iterations):\n",
        "    random_row = filtered_table.sample(n=1)\n",
        "    img_path = random_row['image_path'].values[0]\n",
        "    label = random_row['label'].values[0]\n",
        "    img = load_img(img_path, target_size=(256, 256))\n",
        "    img_array = img_to_array(img)\n",
        "\n",
        "    augmented_images = augmentation_generator.random_transform(img_array)\n",
        "    augmented_data.append({\n",
        "        'image': augmented_images,\n",
        "        'label': label\n",
        "    })\n",
        "\n",
        "  for index, row in filtered_table.iterrows():\n",
        "    label = row['label']\n",
        "    img_path = row['image_path']\n",
        "    img = load_img(img_path, target_size=(256, 256))\n",
        "    img_array = img_to_array(img)\n",
        "\n",
        "    augmented_data.append({\n",
        "        'image': img_array,\n",
        "        'label': label\n",
        "    })\n",
        "\n",
        "  # Convert the augmented data to a new dataframe\n",
        "  augmented_df = pd.DataFrame(augmented_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rVaWIbfJ23k"
      },
      "source": [
        "#### Downsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikYifvApI0bi"
      },
      "outputs": [],
      "source": [
        "# drop rows to downsample\n",
        "dataframe = augmented_df\n",
        "\n",
        "for label in dataframe['label'].unique():\n",
        "  target_label = label\n",
        "\n",
        "  num_rows = len(dataframe[dataframe['label'] == target_label]) - 190\n",
        "\n",
        "  if num_rows > 0:\n",
        "    # Get the indices of rows with the target label\n",
        "    rows_to_drop = dataframe[dataframe['label'] == target_label].sample(num_rows).index\n",
        "\n",
        "    # Drop the specified rows\n",
        "    dataframe = dataframe.drop(rows_to_drop)\n",
        "\n",
        "augmented_df = dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xBLmn64TATu"
      },
      "outputs": [],
      "source": [
        "augmented_df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smEMT6fjV4pC"
      },
      "source": [
        "#### Save data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4AzK-NJ-voS"
      },
      "outputs": [],
      "source": [
        "for _, row in augmented_df.iterrows():\n",
        "  image_np = row['image']\n",
        "  # Convert the numpy array to an image\n",
        "  image = Image.fromarray((np.array(image_np)).astype(np.uint8))\n",
        "\n",
        "  # Display the image using matplotlib\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')  # Turn off axis\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2mVESksVx1i"
      },
      "outputs": [],
      "source": [
        "# Save the dataframe\n",
        "%cd workspace/classifier\n",
        "augmented_df.to_csv('augmented_df.csv', index=False)\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUvQTgDtXVBJ"
      },
      "outputs": [],
      "source": [
        "# Load df\n",
        "%cd workspace/classifier\n",
        "df = pd.read_csv('augmented_df.csv')\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1oBmV3G6Fj_",
        "outputId": "8cea95f9-af53-4198-b596-4b588f11afcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels/workspace/classifier\n",
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels\n"
          ]
        }
      ],
      "source": [
        "# Save only augmented images\n",
        "stacked_augmented_df = np.stack(np.array(augmented_df['image']))\n",
        "%cd workspace/classifier\n",
        "np.save('augmented_array.npy', stacked_augmented_df)\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3FuQ7_cWud9",
        "outputId": "448246f1-1f82-4a3b-fa21-2dd1eda71c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels/workspace/classifier\n",
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels\n"
          ]
        }
      ],
      "source": [
        "# Load augmented images array\n",
        "%cd workspace/classifier\n",
        "loaded_array = np.load('augmented_array.npy')\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bk30X2hdQsF"
      },
      "source": [
        "### Save as csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfWls5LydWuS"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "# Function to convert Numpy arrays to strings\n",
        "def np_array_to_string(array):\n",
        "    return ','.join(map(str, array))\n",
        "\n",
        "# Function to convert strings to Numpy arrays\n",
        "def string_to_np_array(string):\n",
        "    return np.array(ast.literal_eval(string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFlMsghcd-p4"
      },
      "outputs": [],
      "source": [
        "expdf = pd.DataFrame(columns=augmented_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKE94CC5eEpY"
      },
      "outputs": [],
      "source": [
        "expdf = augmented_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FRireleceVOX",
        "outputId": "79e3a99e-ac29-48bd-8812-b344901f2b2e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/Othercomputers/My MacBook Pro/tf-od/panels'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd workspace/classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAIryVITdm2M"
      },
      "outputs": [],
      "source": [
        "# Serialize the NumPy arrays using pickle\n",
        "expdf['image'] = expdf['image'].apply(lambda arr: pickle.dumps(arr))\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "expdf.to_csv('data.csv', index=False)\n",
        "\n",
        "# Load the CSV file back into a DataFrame\n",
        "loaded_df = pd.read_csv('data.csv')\n",
        "\n",
        "# Deserialize the pickled arrays back to NumPy arrays\n",
        "loaded_df['image'] = loaded_df['image'].apply(lambda arr_pickle: pickle.loads(eval(arr_pickle)))\n",
        "\n",
        "# Print the original and loaded arrays for verification\n",
        "print(\"Original DataFrame:\\n\", expdf)\n",
        "print(\"\\nLoaded DataFrame:\\n\", loaded_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU5UAeh_CnBv"
      },
      "source": [
        "### Split into training and testing dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8CgyiyaCms6"
      },
      "outputs": [],
      "source": [
        "def split_data(data_df, train_fraction=0.8):\n",
        "  if train_fraction <= 0 or train_fraction > 1:\n",
        "    raise ValueError(\"train_fraction must be larger than 0 and less than or equal 1\")\n",
        "\n",
        "  # Shuffle the data\n",
        "  unique_labels = data_df['label'].unique()\n",
        "\n",
        "  training = pd.DataFrame(columns=data_df.columns)\n",
        "  testing = pd.DataFrame(columns=data_df.columns)\n",
        "\n",
        "  for label in unique_labels:\n",
        "      label_data = data_df[data_df['label'] == label]\n",
        "\n",
        "      num_testing_rows = max(int(len(label_data) * (1-train_fraction)), 1)\n",
        "\n",
        "      testing_indices = np.random.choice(label_data.index, size=num_testing_rows, replace=False)\n",
        "\n",
        "      testing = pd.concat([testing, label_data.loc[testing_indices]])\n",
        "\n",
        "  # Rows not in testing_df are automatically in training df\n",
        "  training = data_df.drop(testing.index)\n",
        "\n",
        "  print(\"Training dataframe size:\", len(training))\n",
        "  print(\"Testing dataframe size:\", len(testing))\n",
        "\n",
        "  return training, testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxwok0wlnVen"
      },
      "source": [
        "#### Save splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otmGbz8Ck36R",
        "outputId": "9b5d841b-a0aa-4ff4-e3c6-8569a1cf44fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels/workspace/classifier/training\n",
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels/workspace\n"
          ]
        }
      ],
      "source": [
        "# TRAINING\n",
        "# Save only augmented images\n",
        "training_augmented_images = np.stack(np.array(training_df['image']))\n",
        "%cd workspace/classifier/training\n",
        "np.save('images_array.npy', training_augmented_images)\n",
        "%cd ../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1InZLr5mSKu",
        "outputId": "ad2c4c09-0c39-4e60-ee73-14966ccd3152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels/workspace/classifier/training\n",
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels\n"
          ]
        }
      ],
      "source": [
        "# TRAINING\n",
        "# Save the dataframe\n",
        "%cd workspace/classifier/training\n",
        "training_df[['category', 'label']].to_csv('training_df.csv', index=False)\n",
        "%cd ../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPH-U-b6kRdP",
        "outputId": "ce0c7474-224b-46e2-fe03-638f4c85fba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels/workspace/classifier/testing\n",
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels\n"
          ]
        }
      ],
      "source": [
        "# TESTING\n",
        "# Save only augmented images\n",
        "testing_augmented_images = np.stack(np.array(testing_df['image']))\n",
        "%cd workspace/classifier/testing\n",
        "np.save('images_array.npy', testing_augmented_images)\n",
        "%cd ../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7xdwwW7nZ6T",
        "outputId": "7d96c152-4bf4-45a2-b51e-e22a555f9222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels/workspace/classifier/testing\n",
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels\n"
          ]
        }
      ],
      "source": [
        "# TESTING\n",
        "# Save the dataframe\n",
        "%cd workspace/classifier/testing\n",
        "testing_df[['category', 'label']].to_csv('testing_df.csv', index=False)\n",
        "%cd ../../.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbcEQXPnApUY"
      },
      "source": [
        "## Building and training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9N1KlsbLc_n"
      },
      "source": [
        "### Model definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJWDPI6Cyi20"
      },
      "source": [
        "#### Custom model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBTK6Tq9FiHd"
      },
      "outputs": [],
      "source": [
        "num_classes=len(training_df['label'].unique())\n",
        "\n",
        "# Image\n",
        "image_input = Input(shape=(256, 256, 3), name='image_input')\n",
        "conv1 = Conv2D(16, (3, 3), 1, activation='relu', kernel_regularizer=l2(0.3))(image_input)\n",
        "bn1 = BatchNormalization()(conv1)\n",
        "pool1 = MaxPooling2D()(bn1)\n",
        "drop1 = Dropout(0.4)(pool1)\n",
        "\n",
        "conv2 = Conv2D(32, (3, 3), 1, activation='relu', kernel_regularizer=l2(0.3))(drop1)\n",
        "bn2 = BatchNormalization()(conv2)\n",
        "pool2 = MaxPooling2D()(bn2)\n",
        "drop2 = Dropout(0.4)(pool2)\n",
        "\n",
        "conv3 = Conv2D(16, (3, 3), 1, activation='relu', kernel_regularizer=l2(0.3))(drop2)\n",
        "bn3 = BatchNormalization()(conv3)\n",
        "pool3 = MaxPooling2D()(bn3)\n",
        "drop3 = Dropout(0.4)(pool3)\n",
        "\n",
        "flat = Flatten()(drop3)\n",
        "dense1 = Dense(256, activation='relu')(flat)\n",
        "\n",
        "# category input\n",
        "category_input = Input(shape=(1,), name='category_input')\n",
        "dense2 = Dense(64)(category_input)\n",
        "\n",
        "# combine image and category features\n",
        "combined = Concatenate()([dense1, dense2])\n",
        "\n",
        "# output layer for multi-class classification\n",
        "output = Dense(num_classes, activation='softmax')(combined)\n",
        "\n",
        "model = Model(inputs=[image_input, category_input], outputs=output)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DCxNbPByluU"
      },
      "source": [
        "#### AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpnDScsCytio"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.metrics import categorical_crossentropy\n",
        "# from keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers.legacy import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCUnONnvyodm"
      },
      "outputs": [],
      "source": [
        "def AlexNet_regularization(num_classes, learning_rate=0.0005, momentum=0.9, decay=0, regularization=0.2):\n",
        "  model = Sequential()\n",
        "\n",
        "  # layer 1: convolutional layer + max-pooling layer\n",
        "  model.add(Conv2D(filters = 96, kernel_size = (11,11), strides= 4, padding = 'valid', activation='relu', input_shape = (256, 256, 3), kernel_regularizer=l2(regularization)))\n",
        "  model.add(MaxPooling2D(pool_size = (3,3), strides = 2))\n",
        "\n",
        "  # layer 2: convolutional layer + max-pooling layer\n",
        "  model.add(Conv2D(filters = 256, kernel_size = (5,5), padding = 'same', activation = 'relu', kernel_regularizer=L1L2(l2=regularization)))\n",
        "  model.add(MaxPooling2D(pool_size = (3,3), strides = 2))\n",
        "\n",
        "  # layers 3-5: three convolutional layers + 1 max-pooling layer\n",
        "  model.add(Conv2D(filters = 384, kernel_size = (3,3), padding = 'same', activation = 'relu', kernel_regularizer=L1L2(l2=regularization)))\n",
        "  model.add(Conv2D(filters = 384, kernel_size = (3,3), padding = 'same', activation = 'relu', kernel_regularizer=L1L2(l2=regularization)))\n",
        "  model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'same', activation = 'relu', kernel_regularizer=L1L2(l2=regularization)))\n",
        "  model.add(MaxPooling2D(pool_size = (3,3), strides = 2))\n",
        "\n",
        "  # layers 6 - 8: two fully connected hidden layers and one fully connected output layer\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(4096, activation = 'relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(4096, activation = 'relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes, activation = 'softmax'))\n",
        "\n",
        "  optimizer = SGD(lr=learning_rate, momentum=momentum, decay=decay)\n",
        "  # optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.0001)\n",
        "  model.compile(loss = categorical_crossentropy,\n",
        "                optimizer = optimizer,\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def AlexNet(num_classes, learning_rate=1e-04, momentum=0.9, decay=0):\n",
        "  model = Sequential()\n",
        "\n",
        "  # layer 1: convolutional layer + max-pooling layer\n",
        "  model.add(Conv2D(filters = 96, kernel_size = (11,11), strides= 4, padding = 'valid', activation='relu', input_shape = (256, 256, 3)))\n",
        "  model.add(MaxPooling2D(pool_size = (3,3), strides = 2))\n",
        "\n",
        "  # layer 2: convolutional layer + max-pooling layer\n",
        "  model.add(Conv2D(filters = 256, kernel_size = (5,5), padding = 'same', activation = 'relu'))\n",
        "  model.add(MaxPooling2D(pool_size = (3,3), strides = 2))\n",
        "\n",
        "  # layers 3-5: three convolutional layers + 1 max-pooling layer\n",
        "  model.add(Conv2D(filters = 384, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
        "  model.add(Conv2D(filters = 384, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
        "  model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
        "  model.add(MaxPooling2D(pool_size = (3,3), strides = 2))\n",
        "\n",
        "  # layers 6 - 8: two fully connected hidden layers and one fully connected output layer\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(4096, activation = 'relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(4096, activation = 'relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes, activation = 'softmax'))\n",
        "\n",
        "  optimizer = SGD(lr=learning_rate, momentum=momentum, decay=decay)\n",
        "  # optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.0001)\n",
        "  model.compile(loss = categorical_crossentropy,\n",
        "                optimizer = optimizer,\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "fwXFxh_Usk7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALivSeGCicm1"
      },
      "source": [
        "#### AlexNet 2-inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hRshF8dibhc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "def AlexNetMixedInput(num_classes):\n",
        "    # Image input branch\n",
        "    image_input = Input(shape=(256, 256, 3), name='image_input')\n",
        "    x = Conv2D(filters=96, kernel_size=(11, 11), strides=4, padding='valid', activation='relu')(image_input)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=2)(x)\n",
        "    x = Conv2D(filters=256, kernel_size=(5, 5), padding='same', activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=2)(x)\n",
        "    x = Conv2D(filters=384, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
        "    x = Conv2D(filters=384, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
        "    x = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=2)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # category input branch\n",
        "    category_input = Input(shape=(1,), name='category_input')\n",
        "    y = Dense(64, activation='relu')(category_input)\n",
        "\n",
        "    # concatenate image and category branches\n",
        "    combined = tf.keras.layers.concatenate([x, y], axis=-1)\n",
        "\n",
        "    # fully connected layers\n",
        "    z = Dense(4096, activation='relu')(combined)\n",
        "    z = Dropout(0.5)(z)\n",
        "    z = Dense(4096, activation='relu')(z)\n",
        "    z = Dropout(0.5)(z)\n",
        "    output = Dense(num_classes, activation='softmax')(z)\n",
        "\n",
        "    # create the model with two inputs and one output\n",
        "    model = Model(inputs=[image_input, category_input], outputs=output)\n",
        "\n",
        "    # optimizer = SGD(learning_rate=0.00001, momentum=0.6)\n",
        "    optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.0001)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl5aVEcqLiRR"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJlm2S6jWuqU"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSxvH2RIHrZ1",
        "outputId": "5986ac31-8c92-46e5-906f-7570d6aa5e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels/workspace/classifier\n",
            "/content/drive/Othercomputers/My MacBook Pro/tf-od/panels\n"
          ]
        }
      ],
      "source": [
        "# Load augmented images array\n",
        "%cd workspace/classifier\n",
        "loaded_array = np.load('augmented_array.npy')\n",
        "loaded_df = pd.read_csv('augmented_df.csv')\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIVB9GQyHvKw"
      },
      "outputs": [],
      "source": [
        "filtered_df = loaded_df[['label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3-hjAmZJhTN"
      },
      "outputs": [],
      "source": [
        "split_arrays = [np.array(loaded_array[i]) for i in range(loaded_array.shape[0])]\n",
        "filtered_df['image'] = split_arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USBcENY8NPV4",
        "outputId": "52b47430-cc9e-4546-820f-fb3f40e4e527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataframe size: 2263\n",
            "Testing dataframe size: 961\n"
          ]
        }
      ],
      "source": [
        "training_df, testing_df = split_data(filtered_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split data (data processed in the same session, not loaded)"
      ],
      "metadata": {
        "id": "YkHSyRRK__cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df, testing_df = split_data(augmented_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtmBMK4iAOXW",
        "outputId": "625107a9-ddbc-40f2-e713-eb0c291d01c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataframe size: 2018\n",
            "Testing dataframe size: 488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUR9762eWpyx"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iafu2RrGLlHp"
      },
      "outputs": [],
      "source": [
        "data = training_df\n",
        "\n",
        "num_classes = len(data['label'].unique())\n",
        "\n",
        "image_data = np.array(data['image'])\n",
        "image_data = np.stack(image_data)\n",
        "\n",
        "labels = tf.keras.utils.to_categorical(data['label'], num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnUai050pPri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ecb943-4ed8-4383-b2a0-541a1f94564c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Single input Alexnet\n",
        "num_classes = len(data['label'].unique())\n",
        "model = AlexNet(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(image_data, labels, batch_size=32, epochs=200, validation_split=0.2)"
      ],
      "metadata": {
        "id": "yKk90HrGG0tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC0CwmPbpWyk"
      },
      "outputs": [],
      "source": [
        "# Mixed input Alexnet\n",
        "num_classes = len(data['label'].unique())\n",
        "model = AlexNetMixedInput(num_classes)\n",
        "model.fit([image_data, category_numbers], labels, batch_size=32, epochs=500, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving"
      ],
      "metadata": {
        "id": "OgBP6TrkGPG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "import tensorflowjs as tfjs"
      ],
      "metadata": {
        "id": "4KzWdPqVZFYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6acba71-78d7-4456-aea9-52ff4ed80e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TensorFlow Decision Forests 1.6.0 is compatible with the following TensorFlow Versions: ['2.14.0']. However, TensorFlow 2.13.0 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zonokLSE-Wh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a905d748-eb02-4232-b012-2c365625cef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(os.path.join(paths['TRAINED_MODELS'], 'alexnet_v2_no_reg.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = load_model(os.path.join(paths['TRAINED_MODELS'], 'alexnet_v2_82.h5'))\n",
        "# tf.keras.models.save_model(trained_model, os.path.join(paths['TRAINED_MODELS'], 'exports'))"
      ],
      "metadata": {
        "id": "hRCv__0GGCJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model.optimizer.get_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56y7MPNUt5uH",
        "outputId": "7a97234e-86b2-4bd2-d308-a7af98c9e842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'SGD',\n",
              " 'learning_rate': 1e-04,\n",
              " 'decay': 0.0,\n",
              " 'momentum': 0.9,\n",
              " 'nesterov': False}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfjs.converters.save_keras_model(trained_model, os.path.join(paths['TRAINED_MODELS'], 'exports', 'tfjs'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Mtg0pIZ4pk",
        "outputId": "5d33e0a7-3873-457c-8e80-a995217e7db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TFJS\n",
        "!tensorflowjs_converter --input_format keras \\\n",
        "  {os.path.join(paths['TRAINED_MODELS'], 'alexnet_v2_no_reg.h5')} \\\n",
        "  {os.path.join(paths['TRAINED_MODELS'], 'exports', 'tfjs')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy4m8cFUHbIv",
        "outputId": "02300cd5-2d50-4b19-fd24-e0a629e285ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-04 19:34:57.795434: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-04 19:34:57.795484: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-04 19:34:57.795518: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-04 19:34:58.793900: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpdU3ZK-VK28"
      },
      "source": [
        "#### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAX6f_GQZiw0"
      },
      "outputs": [],
      "source": [
        "num_classes = len(training_df['label'].unique())\n",
        "\n",
        "train_images = np.stack(np.array(training_df['image']))\n",
        "train_categories = np.array(training_df['category'])\n",
        "train_labels = tf.keras.utils.to_categorical(training_df['label'], num_classes=num_classes)\n",
        "\n",
        "test_images = np.stack(np.array(testing_df['image']))\n",
        "test_categories = np.array(testing_df['category'])\n",
        "test_labels = tf.keras.utils.to_categorical(testing_df['label'], num_classes=num_classes)\n",
        "\n",
        "learning_rates = [0.0001, 0.0005, 0.00005]\n",
        "momentums = [0.9, 0.8, 0.99]\n",
        "decays = [0, 0.01, 0.001]\n",
        "regularizations = [0.1, 0, 0.25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L3biWcbFWy9E"
      },
      "outputs": [],
      "source": [
        "loops = 0\n",
        "\n",
        "file_path = 'workspace/classifier/training/eval_results.txt'\n",
        "with open(file_path, 'a') as file:\n",
        "  for lr in learning_rates:\n",
        "    for moment in momentums:\n",
        "      for dc in decays:\n",
        "        for reg in regularizations:\n",
        "          model = AlexNet(num_classes=num_classes,\n",
        "                          learning_rate=lr,\n",
        "                          momentum=moment,\n",
        "                          decay=dc,\n",
        "                          regularization=reg)\n",
        "          model.fit(train_images, train_labels, batch_size=32, epochs=200, validation_split=0.2)\n",
        "\n",
        "          predictions = model.predict(test_images)\n",
        "          predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "          # Evaluate the model performance (optional)\n",
        "          accuracy = np.mean(predicted_labels == np.argmax(test_labels, axis=1))\n",
        "\n",
        "          print(\"#########\")\n",
        "          print(\"#########\")\n",
        "          print(f\"Loop number: {loops}\")\n",
        "          print(f\"{lr},{moment},{dc},{reg}: => accuracy: {accuracy}\")\n",
        "          print(\"#########\")\n",
        "          loops += 1\n",
        "\n",
        "          file.write(str([lr, moment, dc, reg, accuracy]) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHfLuNZTA7JM"
      },
      "outputs": [],
      "source": [
        "file_path = 'workspace/classifier/training/eval_results.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        print(line.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEH91O_NA7fW"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKuVhVDyQ009"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load trained model"
      ],
      "metadata": {
        "id": "5u2heJJyUOrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# alexnet_v2_82.h5 yields 82% on test data\n",
        "model = load_model(os.path.join(paths['TRAINED_MODELS'], 'alexnet_v2_82.h5'))"
      ],
      "metadata": {
        "id": "OUfrm4UxUODQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test model"
      ],
      "metadata": {
        "id": "3AiC9veZUSMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = testing_image_data = np.stack(np.array(testing_df['image']))"
      ],
      "metadata": {
        "id": "YumqUpLBZtTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCE7T0-QQloZ",
        "outputId": "58c609a3-7669-489c-a43e-ce0878342d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 1s 39ms/step\n",
            "Accuracy on test data: 0.81\n"
          ]
        }
      ],
      "source": [
        "# Load your Pandas DataFrame\n",
        "data = testing_df\n",
        "\n",
        "# Preprocess your data\n",
        "num_classes = len(data['label'].unique())\n",
        "\n",
        "testing_image_data = np.stack(np.array(testing_df['image']))\n",
        "labels = tf.keras.utils.to_categorical(data['label'], num_classes=num_classes)\n",
        "\n",
        "\n",
        "image_test = testing_image_data\n",
        "label_test = labels\n",
        "\n",
        "# Predict using the trained model\n",
        "predictions = model.predict(image_test)\n",
        "\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Evaluate the model performance\n",
        "accuracy = np.mean(predicted_labels == np.argmax(label_test, axis=1))\n",
        "print(f\"Accuracy on test data: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFDdHXYnZGM0"
      },
      "outputs": [],
      "source": [
        "[labels[prediction] for prediction in predicted_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeYyDaZbRnZ1"
      },
      "outputs": [],
      "source": [
        "test_data_np = (np.array(testing_image_data) * 255).astype(np.uint8)\n",
        "\n",
        "for ind in range(len(predicted_labels)):\n",
        "  print(labels[predicted_labels[ind]])\n",
        "  image_np = test_data_np[ind]\n",
        "\n",
        "  # Convert the numpy array to an image\n",
        "  image = Image.fromarray(image_np)\n",
        "\n",
        "  # Display the image using matplotlib\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IVHnnhKv5Im"
      },
      "outputs": [],
      "source": [
        "# Image input\n",
        "image_input = Input(shape=(256, 256, 3), name='image_input')\n",
        "conv1 = Conv2D(16, (3, 3), 1, activation='relu')(image_input)\n",
        "pool1 = MaxPooling2D()(conv1)\n",
        "\n",
        "conv2 = Conv2D(32, (3, 3), 1, activation='relu')(pool1)\n",
        "pool2 = MaxPooling2D()(conv2)\n",
        "\n",
        "conv3 = Conv2D(16, (3, 3), 1, activation='relu')(pool2)\n",
        "pool3 = MaxPooling2D()(conv3)\n",
        "\n",
        "flat = Flatten()(pool3)\n",
        "dense1 = Dense(256, activation='relu')(flat)\n",
        "\n",
        "# Category input\n",
        "category_input = Input(shape=(1,), name='category_input')\n",
        "dense2 = Dense(64, activation='relu')(category_input)\n",
        "\n",
        "# Combine image and category features\n",
        "combined = Concatenate()([dense1, dense2])\n",
        "\n",
        "# Output layer for multi-class classification\n",
        "output = Dense(num_classes, activation='softmax')(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0WwsJyNt5Qv"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=[image_input, category_input], outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa8Tx1NTwQ6b"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=tf.losses.CategoricalCrossentropy(), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h191p-6At_pO"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimentation"
      ],
      "metadata": {
        "id": "xt75RhDMAno4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_map = [(5, 0), (6, 0), (3, 109), (13, 131), (15, 131), (2, 140), (4, 140), (12, 140),\n",
        "                    (1, 140), (0, 140), (20, 140), (17, 140), (16, 90), (14, 80),\n",
        "                    (18, 60), (11, 50), (19, 20), (7, 20), (9, 10), (8, 10), (10, 10)]"
      ],
      "metadata": {
        "id": "AzYK7giIAo5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_df['label'].value_counts()"
      ],
      "metadata": {
        "id": "0GYDT0gfAuLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(augmented_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "QNw5cekpAz0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dgCUuR8Z2mZp",
        "6MCc_G2F5RIh",
        "6bk30X2hdQsF",
        "Bxwok0wlnVen",
        "zJWDPI6Cyi20",
        "2DCxNbPByluU",
        "ALivSeGCicm1"
      ],
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPDBt2A/auixIxspcHcTqWK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}